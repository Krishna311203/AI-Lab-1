Aim:
Implementation of learning algorithms for an application.

Algorithm:
Entropy is the measure of uncertainty of a random variable, it characterizes the
impurity of an arbitrary collection of examples. The higher the entropy the more the information
content.

1. Start
2. Consider the whole training set as the root
3. Attributes are assumed to be categorical and are distributed recursively according
to values using
statistical methods
4. Find the best attribute and place it on the root node of the tree.
5. Now, split the training set of the dataset into subsets. While making the subset
make sure that each subset of the training dataset has the same value for an attribute. The entropy
increases for each partition.
6. Find leaf nodes in all branches by repeating steps 1 and 2 on each subset
7. End

Program:
import numpy as np
import matplotlib.pyplot as plt 

import pandas as pd  
import seaborn as sns 

%matplotlib inline
from sklearn.datasets import load_boston
boston_dataset = load_boston()
boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)
boston.head()
boston['MEDV'] = boston_dataset.target
boston.isnull().sum()
sns.set(rc={'figure.figsize':(11.7,8.27)})
sns.distplot(boston['MEDV'], bins=30)
plt.show()
correlation_matrix = boston.corr().round(2)
# annot = True to print the values inside the square
sns.heatmap(data=correlation_matrix, annot=True)
plt.figure(figsize=(20, 5))

features = ['LSTAT', 'RM']
target = boston['MEDV']

for i, col in enumerate(features):
    plt.subplot(1, len(features) , i+1)
    x = boston[col]
    y = target
    plt.scatter(x, y, marker='o')
    plt.title(col)
    plt.xlabel(col)
    plt.ylabel('MEDV')

Output:(graph)
Run code in Google Colab.

Result:
Hence, implementation of learning algorithms for an Application has executed successfully.
